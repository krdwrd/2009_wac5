\subsection{DOM Engine}

The choice of DOM engine is central to the implementation.
We reviewed all major engines available today with respect to the requirements listed in \ref{design}:

The KDE Project's KHTML drives the Konquerer browser and some more exotic ones, but lacks a generic multi-platform build process.

This practical limitation is liftet by Apple's fork of KHTML, called WebKit.
It is the underlying engine of Safari browsers on Mac OS X and Windows. % and uses the JavaScriptCore.
There also exists a Qt and a GTK based open source implementation.
While they are quite immature at the moment and not very wildly used, this will change in the future and WepKit will certainly become a valueable option at some point.

While the open source variant of Google's browser, \textit{chromium}, promises superior execution speed by coupling WebKit with it's own V8 JavaScript engine, it suffers from the same problem of not being stable enough yet to serve as reliable platform -
The Linux client for example is barely usable, a Mac client does not exist yet.

We also briefly checked on Presto (Opera) and Trident (Microsoft), but discarded them due to their proprietary nature and lack of suitable APIs.

The Gecko engine (Mozilla Corporation) in conjunction with it's JavaScript implementation Spidermonkey marks a special case:
It implements XUL, the XML User Interface Language, as a way to create feature rich cross-platform applications.
The most prominent of those is the Firefox browser, but also e.g. Thunderbird, Sunbird and Flock are built with XUL.
An add-on system is provided that allows extending the functionality of XUL applications by third-party code that gains full access to the DOM representation, including the XUL part itself.
The proposed {\KrdWrd} backend can be implemented in the same manner as Firefox: provide custom JavaScript and XUL code on top of Mozilla's core XUL Runner. 
Code can easily be shared between a browser add-on and this application and unsupervised operation is trivial to implement in a XUL program.

Given the synergy attainable in the XUL approach and Firefox' popularity amongst users, it was a simple decision to go with Mozilla Gecko for the core DOM implementation.
We note that WebKit's rise and fast pace of development might change that picture in the future.

\subsubsection{Firefox Add-on}

To avoid interference with other add-ons and existing configuration, we suggest creating a new browser profile for use with the Add-on.
For easy use, Firefox' proxy configuration is automatically pointed to a preconfigured host and the user is directed to a special landing page upon successful installation.
Furthermore, the installation binary is digitaly signed, so the user does not have to go through various exception dialogs.


Once installed, the functionality of the Add-on is available via a broom icon in the status bar.
While it offers lots of functions around annotation and corpus selection, it's core feature is simple:
In highlight mode (the broom turns fuchsia) mouse hovering over the page will highlight the text blocks below the cursor.
The block can then be annotated using the context-menu or a keyboard short-cut, which will change its color to the one corresponding to the annotation class.
Figure \ref{f:tut0} shows a fully annotated page and the context-menu.

\begin{figure}
\jss{\includegraphics[width=0.5\textwidth]{tut0}}
	{\includegraphics[width=0.8\textwidth]{tut0}}
\caption{\label{f:tut0}Web pages can be annotated with the {\KrdWrd} Firefox Add-on by hovering over the text by mouse and setting class labels by keyboard short-cut or pop-up menu}
\end{figure}


\subsubsection{XUL Application \label{app}}

The XUL application consists of a thin JavaScript layer on top of Mozilla's XUL Runner.
It mainly uses the XUL browser control to load and render web pages and hooks into its event handlers to catch completed page load events and thelike.
Without greater C level patching, XUL still needs to create a window for all its features to work.
In server applications, we suggest using a virtual display such as Xvfb to fullfil that reqirement.

In operation the application parses the commandline given, which triggers loading of supplied URLs (local or remote) in dedicated browser widgets.
When the load complete event fires, one of several extraction routines is run and results written back to disk.
Implemented extration routines are 
\begin{description}
\item[grab] for simple HTML dumps and screen-shots,
\item[merge] for merging different annotations on the same web page into one in a simple voting scheme, and
\item[pipe] for textual, strutural and visual data for the feature pipelines.
\end{description}

% ens said: if you like:
% \cite{NajorkHeydon2001,ShkapenyukSuel2002}

\subsection{Storage and Control}

Central storage of web pages and annotation data is provided by a database.
Clients access it via CGI scripts executed by a web server while the backend uses python wrapper scripts.

\subsubsection{Web Server}

Server-side logic is implemented by Python CGI scripts, so any web server capable of serving static files and executing CGI scripts is supported.
Users can access the server directly by URL or via the Firefox Add-on menu.
It provides a submission overview as well as a detailed per-corpus submission list.
In conjunction with the Add-on, it controls serving of corpus pages by summing over submissions in the database and randomly selecting a page from those with the least total submission number.
It also serveres the actual HTML data, whereas any embedded objects are served by the separate proxy server.
Furthermore, it controls the tutorial: Users are presented with sample pages and asked to annotate them.
Upon submission, the server compares the user's annotation with a reference annotation stored in the database and generates a page that highlights differences.
The result is delivered back to the user's browser as seen in figure \ref{f:tut1}.

\begin{figure}
\jss{\includegraphics[width=0.5\textwidth]{tut1}}
	{\includegraphics[width=0.8\textwidth]{tut1}}
\caption{\label{f:tut1}During the tutorial, a Visual Diff between the user's submission and the sample data is presented right after submission.
	Here, the annotation from \ref{f:tut0} was wrong in tagging the sub-heading ``ITSS Helpdesk'': the correct annotation (\textit{yellow}) is highlighted in the feedback.}
\end{figure}

\subsubsection{Database}

The database mainly stores the raw HTML code of the corpus pages.
User submissions are vectors of annotation classes, the same length as the number of text nodes in a page.
In addition there is a user mapping table that links internal user ids to external authentication.
Thereby user submissions are anonymized, yet trackable by id.

Given the simple structure of the database model, we choose to use zero-conf database backend \textit{sqlite}.
This should scale up to some thousand corpus pages and users.

It is important to note that any database content must be pre-processed to be encoded in utf-8 only.
Unifing this bit of data representation at the very start is essential to avoid \textit{encoding hell} later in the process.
\subsubsection{Proxy}

Any object contained in the corpus pages needs to be stored and made available to viewers of the page without relying on the original Internet source.

\subsection{Feature Extractors}

The XUL Application extracts information from corpus pages and dumps it into the file-system, to serve as input to specialized feature extractors.
This implementation focuses on feature extraction on those nodes carrying textual content, providing one feature vector per such node.
We therefore generate one feature vector per such node through a linguistic, visual and DOM-tree focused pipeline.

\subsubsection{Text}

For linguistic processing, the Application dumps raw text from the individual nodes, with leading and trailing whitespace removed, converted to UTF-8 where applicable.
External applications can read these data and write back the feature vector resulting from their computation in the same format.

\subsubsection{Structural}

During the Application run, a set of ``DOM-Features'' is directly generated and dumped as feature vector.

Our reference implementation includes features such as depth in the DOM-tree, number of neighboring nodes, ratio text characters to html code characters,
  and some generic document properties as number of links, images, embedded objects and anchors.
We also provide a list of the types of node preceding the current node in the DOM-tree.

Choosing the right DOM properties and apply the right scaling is a non-trivial per-application decision.

\subsubsection{Visual}

\begin{figure}
\includegraphics[width=0.5\textwidth]{vizwrap}
\caption{\label{f:vizwrap}Coordinates of a node's bounding box (straight) and text constituents (dottet) as provided to the visual processing pipeline.}
\end{figure}

For visual analysis, the Application provides full-document screenshots and coordinates of the bounding rectangles of all text nodes.%
\footnote{This Extractor requires at least XULRunner Version 1.9.2 (corresponding to Firefox Version 3.5) which is still in beta at the time of this writing}
When text is not rendered in one straight line, multiple bounding boxes are provided as seen in figure \ref{f:vizwrap}.
This input can be processed by any application suitable for visual feature extraction.

For simple statistics dealing with the coordinates of the bounding boxes, we use a simple Python script to generate basic features such as total area covered in pixel, number of text constituents, their variance in x-coordinates, avarage height and thelike.

Furthermore, we provide a tool chain to use the JAMF framework \cite{Steger08}, a component-based client/server system for building and simulating visual attention models.
The ``CoordRect'' JAMF component generates masks the same size of the web page screenshot based on the node coordinates.
It therefore allows region-wise analysis of the page rendering with the default component set provided by JAMF, which is focused on visual feature extraction.
Results are read by the JAMF Python client and converted into feature vectors on a per-node basis.

Clearly, the components and filter of JAMF model employed or using an entirely different framework for actualy visual feature extraction are per-application decisions to be made.


\subsection{Machine Learner}

Generic input vectors and classes from extractors

\subsection{\label{sec:limitations}Highlights and Limitations}

html, utf, js, iframes
