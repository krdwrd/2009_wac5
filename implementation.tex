\subsection{DOM Engine}


KHTML/WebKit (KDE, Apple), JavaScriptCore or V8 (Google);
The Gecko engine (Mozilla Corporation) and it's JavaScript implementation Spidermonkey;
We briefly checked on Presto (Opera) and Trident (Microsoft), but discarded them due to their proprietary nature.

\subsubsection{Firefox Add-on}

To avoid interference with other add-ons and existing configuration, we suggest creating a new browser profile for use with the Add-on.
For easy use, Firefox' proxy configuration is automatically pointed to a preconfigured host and the user is directed to a special landing page upon successful installation.
Furthermore, the installation binary is digitaly signed, so the user does not have to go through various exception dialogs.


Once installed, the functionality of the Add-on is available via a broom icon in the status bar.
While it offers lots of functions around annotation and corpus selection, it's core feature is simple:
In highlight mode (the broom turns fuchsia) mouse hovering over the page will highlight the text blocks below the cursor.
The block can then be annotated using the context-menu or a keyboard short-cut, which will change its color to the one corresponding to the annotation class.
Figure \ref{f:tut0} shows a fully annotated page and the context-menu.

\begin{figure}
\jss{\includegraphics[width=0.5\textwidth]{tut0}}
	{\includegraphics[width=0.8\textwidth]{tut0}}
\caption{\label{f:tut0}Web pages can be annotated with the \KrdWrd Firefox Add-on by hovering over the text by mouse and setting class labels by keyboard short-cut or pop-up menu}
\end{figure}


\subsubsection{XUL Application \label{app}}

The XUL application (or \textit{app} for short) 
Dumper, Grabber, Crawler

% ens said: if you like:
% \cite{NajorkHeydon2001,ShkapenyukSuel2002}

\subsection{Storage and Control}


database and web server. 
html hosted on own server, with baseurl; all other data through proxy; initially populated by the grabber described in \ref{app}.
user annotations list of tags, one entry for every text node, in dom tree order;


\subsubsection{Web Server}

Server-side logic is implemented by Python CGI scripts, so any web server capable of serving static files and executing CGI scripts is supported.
Users can access the server directly by URL or via the Firefox Add-on menu.
It provides a submission overview as well as a detailed per-corpus submission list.
In conjunction with the Add-on, it controls serving of corpus pages by summing over submissions in the database and randomly selecting a page from those with the least total submission number.
It also controls the tutorial: Users are presented with sample pages and asked to annotate them.
Upon submission, the server compares the user's annotation with a reference annotation stored in the database and generates a page that highlights the differences.
The result is delivered back to the user's browser as seen in figure \ref{tut1}.

\begin{figure}
\jss{\includegraphics[width=0.5\textwidth]{tut1}}
	{\includegraphics[width=0.8\textwidth]{tut1}}
\caption{\label{f:tut1}During the tutorial, a Visual Diff between the user's submission and the sample data is presented right after submission.
	Here, the annotation from \ref{f:tut0} was wrong in tagging the sub-heading ``ITSS Helpdesk'': the correct annotation (\textit{yellow}) is highlighted in the feedback.}
\end{figure}

\subsubsection{Database}

The database mainly stores the raw HTML code of the corpus pages.
User submissions are vectors of annotation classes, the same length as the number of text nodes in a page.
In addition there is a user mapping table that links internal user ids to external authentication.
Thereby user submissions are anonymized, yet trackable by id.

Given the simple structure of the database model, we choose to use zero-conf database backend \textit{sqlite}.
This should scale up to some thousand corpus pages and users.

\subsubsection{Proxy}

Any object contained in the corpus pages needs to be stored and made available to viewers of the page without relying on the original Internet source.

\begin{figure}
\jss{\includegraphics[width=0.5\textwidth]{add}}
	{\includegraphics[width=0.8\textwidth]{add}}
\caption{IFrames with dynamic URLs which usually come from advertisements are blocked as a nice side-effect of the Proxy setup}
\end{figure}

\subsection{Feature Extractors}

The XUL Application extracts information from corpus pages and dumps it into the file-system, to serve as input to specialized feature extractors.
This implementation focuses on feature extraction on those nodes carrying textual content, providing one feature vector per such node.
We therefore generate one feature vector per such node through a linguistic, visual and DOM-tree focused pipeline.

\subsubsection{Text}

For linguistic processing, the Application dumps raw text from the individual nodes, with leading and trailing whitespace removed, converted to UTF-8 where applicable.
External applications can read these data and write back the feature vector resulting from their computation in the same format.

\subsubsection{Structural}

During the Application run, a set of ``DOM-Features'' is directly generated and dumped as feature vector.

Our reference implementation includes features such as depth in the DOM-tree, number of neighboring nodes, ratio text characters to html code characters,
  and some generic document properties as number of links, images, embedded objects and anchors.
We also provide a list of the types of node preceding the current node in the DOM-tree.

Choosing the right DOM properties and apply the right scaling is a non-trivial per-application decision.

\subsubsection{Visual}

\begin{figure}
\includegraphics[width=0.5\textwidth]{vizwrap}
\caption{\label{f:vizwrap}Coordinates of a node's bounding box (straight) and text constituents (dottet) as provided to the visual processing pipeline.}
\end{figure}

For visual analysis, the Application provides full-document screenshots and coordinates of the bounding rectangles of all text nodes.%
\footnote{This Extractor requires at least XULRunner Version 1.9.2 (corresponding to Firefox Version 3.5) which is still in beta at the time of this writing}
When text is not rendered in one straight line, multiple bounding boxes are provided as seen in figure \ref{f:vizwrap}.
This input can be processed by any application suitable for visual feature extraction.

For simple statistics dealing with the coordinates of the bounding boxes, we use a simple Python script to generate basic features such as total area covered in pixel, number of text constituents, their variance in x-coordinates, avarage height and thelike.

Furthermore, we provide a tool chain to use the JAMF framework \cite{Steger08}, a component-based client/server system for building and simulating visual attention models.
The ``CoordRect'' JAMF component generates masks the same size of the web page screenshot based on the node coordinates.
It therefore allows region-wise analysis of the page rendering with the default component set provided by JAMF, which is focused on visual feature extraction.
Results are read by the JAMF Python client and converted into feature vectors on a per-node basis.

Clearly, the components and filter of JAMF model employed or using an entirely different framework for actualy visual feature extraction are per-application decisions to be made.


\subsection{Machine Learner}

Generic input vectors and classes from extractors

\subsection{\label{sec:limitations}Highlights and Limitations}

html, utf, js, iframes
