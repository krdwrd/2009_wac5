
\begin{abstract}
Algorithmic processing of Web content mostly works on textual contents, neglecting visual information.
Annotation tools largely share this deficit as well.

We specify requirements for an architecture to overcome both problems and propose an implementation, the \KrdWrd~system.
It uses the Gecko rendering engine for both annotation and feature extraction, providing unified data access in every processing step.
Stable data storage and collaboration control scripts for group annotations of massive corpora are provided via a Web interface coupled with a HTTP proxy.
A modular interface allows for linguistic and visual data feature extractor plugins.

The implementation is suitable for many tasks in the \textit{Web as corpus} domain and beyond.
\end{abstract}

\section{Introduction}
\input{problem}

\section{Design\label{design}}
\input{design}

\section{Implementation\label{impl}}

We maintain the implementation in a source code repository at \url{http://krdwrd.org}.
The documentation includes pointers to the required external software.

This section will first describe the DOM engine and its use by browser and back-end application (\ref{dom}),
then the details of the implementation of central storage and control (\ref{server}), and will end with listing possible feature extractors for the back-end (\ref{extract}).


\input{implementation}

\section{Case Study\label{casestudy}}

The current implementation comprises an extensive system for pre-processing and automated cleaning of Web pages,
i.e.~a typical Web-as-corpus task, 
where users are provided with accurate Web page presentations and annotation utilities in a typical browsing environment,
while supervised machine learning algorithms also operate on representations of the visual rendering of Web pages. 

The sequence of steps includes
corpus creation and acquisition of hand-annotated training data on that corpus (\ref{datagather}), 
feature extraction (\ref{extrapipe}), 
training of a classifier and producing annotated test results (\ref{experiment}).

The underlying data, tools, and programs are bundled with the {\KrdWrd} distribution as usage example.


\subsection{Data Acquisition\label{datagather}}

Gathering a set of sample pages is the first step before utilizing people to tag new data.
Therefore, we acquired a new corpus named \textit{Canola} by using the BootCaT \cite{bootcat} tool to produce a URL list from the seed terms in Table \ref{t:seed} using the Yahoo search engine. 

\begin{table}
\centering
\jss{
\caption{BootCaT seed terms for \textit{Canola} corpus}\bigskip}{
\captionabove{BootCaT seed terms for \textit{Canola} corpus}\sffamily}
\begin{tabular}[h]{ccc}
        history
&        coffee 
&        salt \\
        spices 
&        trade road
&        toll \\
        metal
&        silk 
&        patrician \\
        pirate 
&        goods
&        merchant 
\end{tabular}
\label{t:seed}
\end{table}

To populate the proxy, we ran the Application on every URL once and also extracted the textual content of the pages.
We then filtered for text lengths between 500 and 6,000 characters \footnote{\ldots for Chinese these numbers had to be cut down to 50 and 600, however.} and ran the Application once again, this time dumping the raw HTML code of the pages in UTF-8 format.
During this second pass, the proxy is switched to block access to external sources.
This ensures that no dynamic external content makes it into the corpus data, while letting innocent content pass.
See Figure \ref{f:iframes} for an example.

The resulting HTML is post-processed to ensure that references and encodings are consistent:
The head tag is expanded by a \texttt{<base href="\textit{original url}" />} line, so a browser later viewing the dumped HTML will request embedded objects by their original URLs, which can then be served by the proxy.
After removing any non-UTF-8 encoding hints, the data is fed into the database's page table, with a unique page id and the corpus id.

\begin{figure}
\jss{\includegraphics[width=0.5\textwidth]{add}}
	{\includegraphics[width=0.8\textwidth]{add}}
\caption{\label{f:iframes}IFrames with dynamic URLs which usually come from advertisements are blocked as a nice side-effect of the Proxy setup.}
\end{figure}

The pre-processed data is now ready to be processed by annotators.
For gathering training data, students were asked to go through the ten Web page annotation tutorial once
-- to get acquainted with the annotation tool, i.e.~the Add-on, and different aspects of how to apply the guidelines 
\footnote{A refined version of the official `CLEANEVAL: Guidelines for annotators' \url{http://cleaneval.sigwac.org.uk/annotation_guidelines.html} available at \url{https://krdwrd.org/manual/html}.}
to real-world Web pages -- 
and then annotate pages from the \textit{Canola} corpus as part of an homework assignment.
The annotation process consisted of tagging text on Web pages with \emph{three} tags `good', `bad', and `uncertain'.

Over the course of two weeks, about 60 students provided a total average of 7.75 annotations per page.
As the time data in Figure \ref{f:userstats} suggests, users learn quickly; 
Average per-page annotation times drop well below three minutes after some training.
The tutorial with its ten pages took on average 22 minutes to complete;
note however, these pages were shortened and stripped down to illustrate particular aspects of Web pages.

Integration of the Add-on in users' environments was flawless and we did not receive any reports of usability or general handling problems.
Manual inspection of submissions also did not show any anomalies, but to the contrary, indicated that submitters took great care to provide adequate annotations (c.f.~Figure \ref{f:merge}).

\begin{figure}[h]
\centering
\jss{
\includegraphics[width=0.5\textwidth]{timespentonpage}
}{
\includegraphics[width=0.8\textwidth]{timespentonpage}
}
\caption{
\label{f:userstats}
\label{ens1}
	Time spent for annotation of a single Web page across all annotators of the \textit{Canola} corpus.}
\end{figure}

The data obtained from user annotations was next merged into a single corpus using the Application's \textit{merge} function (c.f.~\ref{merge}), resulting in a total of 216 corpus pages, each backed by up to 8 user submissions.
Different treatment of JavaScript on the client side resulted in partial misalignment on some pages:
dynamic client code had inserted or re-ordered nodes in some instance while not in others.
We extended the merge procedure to accept some fuzziness in node matching, but still lost data from about 5\% of submissions that could not be re-aligned.
Until this problem is solved, we turn off JavaScript for Web content via the Firefox Add-On.
Note that attaching unique IDs to text nodes is only a partial solution to this problem:
A common JavaScript idiom is to clone an existing element and to populate it with new content, ultimately leading to different nodes with the same ``unique'' ID. 

\begin{figure}[h]
\centering
\jss{
\includegraphics[width=0.5\textwidth]{agreementonpages}
}{
\includegraphics[width=0.8\textwidth]{agreementonpages}
}
\caption{
\label{f:merge}
\label{ens2}
	 Fleiss's multi-$\pi$ agreement \cite{artsteinpoesio2008} between submissions for pages over the \textit{Canola} corpus.}
\end{figure}


\subsection{Extraction Pipeline\label{extrapipe}}

Feature Extraction commences by running the {\KrdWrd} application extraction pipeline over the merged data obtained during annotation. 
For the \textit{Canola} corpus' 216 pages, it took 2.5 seconds on average per page to generate text (2.5 million characters total), DOM information (46575 nodes total), screen-shots (avg.~size 997x4652 pixels) and a file with the annotation target class for each text node.

We only used the stock {\KrdWrd} features on the DOM tree and visual pipeline.
For computing textual features, we borrowed Victor's \cite{spoustamarekpecina2008} text feature extractor.

\subsection{Experiment\label{experiment}}

We used the data gathered by the feature extraction for training a Support Vector Machine \cite{libsvm}.
We used an RBF kernel with optimal parameters determined by a simple grid search to create ad-hoc models on a per-pipeline basis.
The total number of feature vectors corresponded to the number of text nodes in the corpus and was 46575.
Vector lengths for the different pipelines and test results from 10-fold cross validation are shown in Table \ref{t:res}.

Although the results for the single pipelines look quite promising -- especially the surprisingly good performance of the visual pipeline given its limited input -- combinations of feature sets in a single SVM model perform only marginally better.
We therefore suggest running separate classifiers on the feature sets and only merging their results later, possibly in a weighted voting scheme.
DOM features would certainly benefit most from e.g.~a classifier that can work on structured data.

\begin{table}
\jss{
\caption{10-fold cross validated classification test results for different combinations of the textual (cl), DOM-property based (dom) and visual (viz) pipelines on the \textit{Canola} data set obtained using stock SVM regression with a RBF kernel.
\label{t:res}
}}{
\captionabove{10-fold cross validated classification test results for different combinations of the textual (cl), DOM-property based (dom) and visual (viz) pipelines on the \textit{Canola} data set obtained using stock SVM regression with a RBF kernel.
\label{t:res}}
\sffamily}
\centering
\begin{tabular}[h]{l|c|rrr}
Modules & \jss{Feat.}{Number of Features} & \jss{Acc.}{Accuracy} & \jss{Prec.}{Precision} & \jss{Recall}{Recall} \\
\hline
cl         & 21 & 86\% & 61\% & 76\% \\
dom *      & 13 & 65\% & 64\% & 56\% \\
viz *      &  8 & 86\% & 64\% & 82\% \\
cl dom *   & 34 & 67\% & 74\% & 57\% \\
dom viz *  & 21 & 67\% & 72\% & 59\% \\
cl viz     & 29 & 86\% & 63\% & 78\% \\
cl dom viz & 42 & 68\% & 76\% & 58\% \\
\end{tabular}

* data obtained by training on reduced number of input vectors.
\end{table}


\subsection{Inspecting Classifier Results}

The classification results can be back-projected into the DOM-trees using the Application's \textit{diff} function.
As in the tutorial for annotators, it produces a visual diff, showing where the classifier failed.
Note that these results are just Web pages, so they can be viewed anywhere without the help of the Add-on.
This quickly turned out to be a valuable tool for evaluation of classification results.

\section{\label{sec:limitations}Conclusion\label{conc}}

Employing {\KrdWrd} in the \textit{Canola} case study showed that we achieved what we set out for and gave some valuable experience for possible improvements:

The {\KrdWrd} Firefox Add-On is the first tool for Web page annotation that integrates flawlessly into a users daily browsing experience.
It is unobtrusive and has a simple and intuitive user interface.
Users quickly learn how to annotate and produce quite uniform results, given sufficient annotation guidelines.

The {\KrdWrd} application and supporting infrastructure are a reliable platform under a real-world usage scenario.
By decoding any input data to UTF-8 at the moment it enters the system and ensuring that we explicitly deliver UTF-8 exclusively throughout the system, we circumvented all usual encoding problems.

The overall handling of JavaScript is not satisfactory.
To address the diversions between submits occurring after dynamic client-side JavaScript execution on different clients, the Add-on could hook into the node creation and clone processes.
They could be suppressed entirely or newly created nodes could grow a special id tag to help identifying them later.

For result analysis, we would like to expand the visual diff generated from classification results.
Showing results from separate runs on different subsets of the data or different parameters on one page would facilitate manual data inspection.
Presenting selected feature values per node might also help in developing new feature extractors, especially in the DOM context.

Furthermore, we would like to integrate the JAMF framework \cite{steger08}, a component-based client/server system for building and simulating visual attention models, into the tool chain.
This would allow for features based on the analysis of the rendered pages akin to how humans perceive these pages while browsing.

Summarizing, we designed and implemented an architecture for holistic treatment of Web pages in classification tasks.
We demonstrated that the {\KrdWrd} system can be used to automatically build an annotated corpus from user submissions.
We also showed the broad set of features for text, structure and imagery it can help to extract, and how their contribution to classification can be assessed graphically.

\review{
%\section*{Acknowledgments}

}

