
\begin{abstract}
Algorithmic processing of web content mostly works on textual contents, neglecting visual information.
Annotation tools mostly share this deficit as well.

We specify requirements for an architecture to overcome both problems and propose an implementation, the \KrdWrd~system.
It uses the Gecko rendering engine both for annotation and for feature extraction, providing unified data access in any processing step.
Stable data storage and collaboration control scripts for group annotations of massive corpora are provided via a web interface coupled with a HTTP proxy.
A modular interface allows plugging in feature extractors for linguistic and visual data.

The implementation is suitable for many tasks in the web as corpus domain and beyond.
\end{abstract}

\section{Introduction}
\input{problem}

\section{Design\label{design}}
\input{design}

\section{Implementation\label{impl}}

We maintain the implementation in a source code repository at \url{http://krdwrd.org}.
The documentation includes pointers to required external software.

\input{implementation}

\section{Case Study\label{casestudy}}

We tested the implementation on a typical web-as-corpus task, including corpus creation, user annotation, feature extraction, traning a classifier and producing annotated test results.
The underlying data and programs are bundled with the {\KrdWrd} distribution as usage example.

\subsection{Data Acquisition\label{datagather}}

We acquired a new corpus named \textit{Canola} by using the BootCat tool to produce an URL list from the seed terms in table \ref{t:seed} using the Yahoo search engine. 

To populate the proxy, we run the Application overy every URL once and also extracted the pages' text.
We then filtered for text lengths between 500 and 5000 characters and run the Application once again, this time dumping the raw HTML code of the pages in UTF-8 format.
During this second pass, the proxy is switched to block access to external sources.
This makes sure that no dynamic external content makes it into the corpus data, while letting innocent content pass.
See figure \ref{f:iframes} for an example.


\begin{figure}
\jss{\includegraphics[width=0.5\textwidth]{add}}
	{\includegraphics[width=0.8\textwidth]{add}}
\caption{\label{f:iframes}IFrames with dynamic URLs which usually come from advertisements are blocked as a nice side-effect of the Proxy setup}
\end{figure}

The resulting HTML is post-processed to ensure references and encodings are consistent:
The head tag is expanded by a \textbf{<base href="\textit{original url}" />} line, so a browser later viewing the dumped HTML will request embedded objects from their original URLs, which can then be served by the proxy.
After removing any non-UTF-8 encoding hints, the data is fed into the database's page table, with a unique page id and the corpus id.

\begin{table}
\label{t:seed}
\centering
\jss{
\caption{BootCaT seed terms for \textit{Canola} corpus}\bigskip}{
\captionabove{BootCaT seed terms for \textit{Canola} corpus}\sffamily}
\begin{tabular}[h]{ccc}
        history
&        coffee 
&        salt \\
        spices 
&        trade road
&        toll \\
        metal
&        silk 
&        patrician \\
        pirate 
&        goods
&        merchant 
\end{tabular}
\end{table}

For gathering annotation data, sixty students were asked to go through the 10 web page annotation tutorial once and then annotate at least 20 pages from the corpus as part of a lecture homework.
They were provided documentation containing a ten-page manual for the Add-on and annotation guidelines.

TODO: ens, some minimal stats here

students: fast, no probs with tool

merge

\subsection{Extraction Pipeline}

Feature Extraction commences by running the {\KrdWrd} application extraction pipeline over the merged data obtain from annotation. 
For the \textit{canola} corpus' 216 pages, it took $2.5sec$ on average to generate text (2.5m characters total), DOM information (46575 nodes total) and screen-shots (avg. size 997x4652px).

We only used the stock {\KrdWrd} features on the DOM tree and visual pipeline, with a simple JAMF graph as a showcase (c.f. figure \ref{f:jamfgraph}).
For computing textual features, we stole Victors gut.

\begin{figure}[h]
\centering
\jss{
\includegraphics[width=0.4\textwidth]{jamfgraph}
}{
\includegraphics[width=0.6\textwidth]{jamfgraph}
}
\label{f:jamfgraph}
\caption{The simple JAMF graph used for the case study}
\end{figure}

\subsection{Inspecting Classifier Results}

As a showcase, we used the data gathered in \ref{datagather} and trained a Support Vector Machine \cite{libsvm} using an RBF kernel with optimal parameters determined by a simple grid search.
The total number of feature vectors was $44444$. Vector lengths for the different pipelines and test results from 10-fold cross validation are shown in table \ref{t:res}.

\begin{table}
\label{t:res}
\jss{
\caption{Classification Results for \textit{Canola} data set\\}}{
\captionabove{Classification Results for {\it Canola} data set}}
\jss{}{\sffamily\centering}
\begin{tabular}[h]{l|c|rrr}
Modules & \jss{Feat.}{Number of Features} & \jss{Acc.}{Accuracy} & \jss{Prec.}{Precision} & \jss{Recall}{Recall} \\
\hline
cl         & 21 & 84\% & 60\% & 70\% \\
dom        & 13 & 65\% & 64\% & 56\% \\
viz        &  8 & 86\% & 64\% & 82\% \\
cl dom     & 34 & 67\% & 74\% & 57\% \\
dom viz    & 21 & 67\% & 72\% & 59\% \\
cl viz     & 29 & 85\% & 60\% & 74\% \\
cl dom viz & 42 & 68\% & 76\% & 58\% \\
\end{tabular}
\end{table}

The results can be back-projected into the DOM-trees using the Application.
As in the tutorial for annotators, it produces a visual diff, showing where the classifier failed.
Note that these results are just web pages, so they can be viewed anywhere without the help of the Add-on.

\section{Conclusion\label{conc}}

We have designed and implemented an architecture for holistic treatment of web pages in classification tasks.
We demonstrated that the KrdWrd system can be used to automatically build an annotated corpus from user submissions.
We also showed its broad set of extractable features, text, structure and imagery, and their contribution to classification.

\subsection{Future Work}


activate JavaScript, but hook into node create and clone processes.

For result analysis, we would like to expand the visual diff generated from classification results.
Showing results from separate runs on different subsets of the data or different parameters on one page would facilitate manual data inspection.
Presenting selected feature values per node might also help in developing new feature extractors, especially in the DOM context.

\subsection{Application}

introduced, shown to work, now you get started.

\review{
\section*{Acknowledgments}

}

