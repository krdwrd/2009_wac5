
\begin{abstract}
Algorithmic processing of Web content mostly works on textual contents, neglecting visual information.
Annotation tools mostly share this deficit as well.

We specify requirements for an architecture to overcome both problems and propose an implementation, the \KrdWrd~system.
It uses the Gecko rendering engine for both annotation and feature extraction, providing unified data access in every processing step.
Stable data storage and collaboration control scripts for group annotations of massive corpora are provided via a Web interface coupled with a HTTP proxy.
A modular interface allows for linguistic and visual data feature extractor plugins.

The implementation is suitable for many tasks in the \textit{Web as corpus} domain and beyond.
\end{abstract}

\section{Introduction}
\input{problem}

\section{Design\label{design}}
\input{design}

\section{Implementation\label{impl}}

We maintain the implementation in a source code repository at \url{http://krdwrd.org}.
The documentation includes pointers to the required external software.

This section will first describe the DOM engine and its use by browser and back-end application (\ref{dom}),
then the details of the implementation of central storage and control (\ref{server}), and will end with listing possible feature extractors for the back-end (\ref{extract}).


\input{implementation}

\section{Case Study\label{casestudy}}

We tested the implementation on a typical Web-as-corpus task, including corpus creation, user annotation, feature extraction, training a classifier and producing annotated test results.
The underlying data and programs are bundled with the {\KrdWrd} distribution as usage example.

\subsection{Data Acquisition\label{datagather}}

We acquired a new corpus named \textit{Canola} by using the BootCaT \cite{bootcat} tool to produce a URL list from the seed terms in table \ref{t:seed} using the Yahoo search engine. 

\begin{table}
\centering
\jss{
\caption{BootCaT seed terms for \textit{Canola} corpus}\bigskip}{
\captionabove{BootCaT seed terms for \textit{Canola} corpus}\sffamily}
\begin{tabular}[h]{ccc}
        history
&        coffee 
&        salt \\
        spices 
&        trade road
&        toll \\
        metal
&        silk 
&        patrician \\
        pirate 
&        goods
&        merchant 
\end{tabular}
\label{t:seed}
\end{table}


To populate the proxy, we ran the Application on every URL once and also extracted the pages' text.
We then filtered for text lengths between 500 and 6,000 characters and ran the Application once again, this time dumping the raw HTML code of the pages in UTF-8 format.
During this second pass, the proxy is switched to block access to external sources.
This ensures that no dynamic external content makes it into the corpus data, while letting innocent content pass.
See figure \ref{f:iframes} for an example.


The resulting HTML is post-processed to ensure that references and encodings are consistent:
The head tag is expanded by a \texttt{<base href="\textit{original url}" />} line, so a browser later viewing the dumped HTML will request embedded objects by their original URLs, which can then be served by the proxy.
After removing any non-UTF-8 encoding hints, the data is fed into the database's page table, with a unique page id and the corpus id.

\begin{figure}
\jss{\includegraphics[width=0.5\textwidth]{add}}
	{\includegraphics[width=0.8\textwidth]{add}}
\caption{\label{f:iframes}IFrames with dynamic URLs which usually come from advertisements are blocked as a nice side-effect of the Proxy setup.}
\end{figure}

For gathering annotation data, students were asked to go through the ten Web page annotation tutorial once and then annotate pages from the \textit{Canola} corpus as part of a class assignment.
They were provided documentation containing a ten-page manual for the Add-on and annotation guidelines.

Over the course of two weeks, about 60 students provided a total average of 7.75 annotations per page.
As the time data in figure \ref{f:userstats} suggests, users learn quickly; 
Average per-page annotation times drop well below three minutes after some training.
The tutorial with its 10 pages took on average 22 minutes to complete.

Integration of the Add-on in users' environments was flawless and we did not receive any reports about usability or general handling problems.
Manual inspection of submissions also did not show any anomalies, to the contrary, indicated that submitters took great care to provide adequate annotations.

\begin{figure}[h]
\centering
\jss{
\includegraphics[width=0.5\textwidth]{timespentonpage}
}{
\includegraphics[width=0.8\textwidth]{timespentonpage}
}
\caption{
\label{f:userstats}
\label{ens1}
	Time spent for annotation of a single Web page across all annotators of the \textit{Canola} corpus.}
\end{figure}

The data obtained from user annotations was next merged into a single corpus using the Applications \textit{merge} function (c.f.~\ref{merge}), resulting in a total of 216 corpus pages, each backed by up to 8 user submissions.
Different treatment of JavaScript on the client side resulted in partial misalignment on some pages:
dynamic client code had inserted or re-ordered nodes in some instance while not in others.
We extended the merge procedure to accept some fuzziness in node matching, but still lost data from about 5\% of submissions that could not be re-aligned.
Until this problem is solved, we recommend turning off JavaScript for Web content via the Firefox Add-On.
Note that attaching unique IDs to text nodes is only a partial solution to this problem:
A common JavaScript idiom is to clone an existing element and to populate it with new content, ultimately leading to different nodes with the same ``unique'' ID. 

\begin{figure}[h]
\centering
\jss{
\includegraphics[width=0.5\textwidth]{agreementonpages}
}{
\includegraphics[width=0.8\textwidth]{agreementonpages}
}
\caption{
\label{f:merge}
\label{ens2}
	Agreement between submissions for pages over the \textit{Canola} corpus.}
\end{figure}


\subsection{Extraction Pipeline}

Feature Extraction commences by running the {\KrdWrd} application extraction pipeline over the merged data obtained during annotation. 
For the \textit{Canola} corpus' 216 pages, it took 2.5 seconds on average per page to generate text (2.5 million characters total), DOM information (46575 nodes total), screen-shots (avg.~size 997x4652 pixels) and a file with the annotation target class for each text node.

We only used the stock {\KrdWrd} features on the DOM tree and visual pipeline.
For computing textual features, we borrowed Victor's \cite{spoustamarekpecina2008} text feature extractor.

\subsection{Experiment}

We used the data gathered by the feature extraction for training a Support Vector Machine \cite{libsvm}.
We used an RBF kernel with optimal parameters determined by a simple grid search to create ad-hoc models on a per-pipeline basis.
The total number of feature vectors corresponded to the number of text nodes in the corpus and was 46575.
Vector lengths for the different pipelines and test results from 10-fold cross validation are shown in table \ref{t:res}.

Although the results for the single pipelines look quite promising -- especially the visual pipeline performed surprisingly well given its limited input -- combinations of feature sets in a single SVM model perform only marginally better.
We therefore suggest running separate classifiers on the feature sets and only merging their results later, possibly in a weighted voting scheme.
DOM features would certainly benefit most from e.g.~a classifier that can work on structured data.

\begin{table}
\jss{
\caption{10-fold cross validated classification test results for different combinations of the textual (cl), DOM-property based (dom) and visual (viz) pipelines on the \textit{Canola} data set obtained using stock SVM regression with a RBF kernel.
\label{t:res}
}}{
\captionabove{10-fold cross validated classification test results for different combinations of the textual (cl), DOM-property based (dom) and visual (viz) pipelines on the \textit{Canola} data set obtained using stock SVM regression with a RBF kernel.
\label{t:res}}
\sffamily}
\centering
\begin{tabular}[h]{l|c|rrr}
Modules & \jss{Feat.}{Number of Features} & \jss{Acc.}{Accuracy} & \jss{Prec.}{Precision} & \jss{Recall}{Recall} \\
\hline
cl         & 21 & 86\% & 61\% & 76\% \\
dom *      & 13 & 65\% & 64\% & 56\% \\
viz *      &  8 & 86\% & 64\% & 82\% \\
cl dom *   & 34 & 67\% & 74\% & 57\% \\
dom viz *  & 21 & 67\% & 72\% & 59\% \\
cl viz     & 29 & 86\% & 63\% & 78\% \\
cl dom viz & 42 & 68\% & 76\% & 58\% \\
\end{tabular}

* data obtained by training on reduced number of input vectors.
\end{table}


\subsection{Inspecting Classifier Results}

The classification results can be back-projected into the DOM-trees using the Application's \textit{diff} function.
As in the tutorial for annotators, it produces a visual diff, showing where the classifier failed.
Note that these results are just Web pages, so they can be viewed anywhere without the help of the Add-on.
This quickly turned out to be a valuable tool for evaluation of classification results.

\section{\label{sec:limitations}Conclusion\label{conc}}

Employing {\KrdWrd} in the \textit{Canola} case study showed that we achieved what we set out for and gave some valuable experience for possible improvements:

The {\KrdWrd} Firefox Add-On is the first tool for Web page annotation that integrates flawlessly into a users daily browsing experience.
It is unobtrusive and has a simple and intuitive user interface.
Users quickly learn how to annotate and produce quite uniform results, given sufficient annotation guidelines.

The {\KrdWrd} application and supporting infrastructure are a reliable platform under a real-world usage scenario.
By decoding any input data to UTF-8 at the moment it enters the system and ensuring that we explicitly deliver UTF-8 exclusively throughout the system, we circumvented all usual encoding problems.

The overall handling of JavaScript is not satisfactory.
To address the diversions between submits occurring after dynamic client-side JavaScript execution on different clients, the Add-on could hook into the node creation and clone processes.
They could be suppressed entirely or newly created nodes could grow a special id tag to help identifying them later.

For result analysis, we would like to expand the visual diff generated from classification results.
Showing results from separate runs on different subsets of the data or different parameters on one page would facilitate manual data inspection.
Presenting selected feature values per node might also help in developing new feature extractors, especially in the DOM context.

Furthermore, we would like to integrate the JAMF framework \cite{Steger08}, a component-based client/server system for building and simulating visual attention models, into the tool chain.
This would allow for features based on the analysis of the rendered pages akin to how humans perceive these pages while browsing.

Summarizing, we designed and implemented an architecture for holistic treatment of Web pages in classification tasks.
We demonstrated that the {\KrdWrd} system can be used to automatically build an annotated corpus from user submissions.
We also showed the broad set of features for text, structure and imagery it can help to extract, and how their contribution to classification can be assessed graphically.

\review{
%\section*{Acknowledgments}

}

